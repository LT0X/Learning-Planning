# 研0 学习计划

**前言：初步敲定计划，可能计划会根据学习的情况进行动态的调整。**

### 第一周(5.12~5.18)

- [x] python相关语法以及常用开发环境的配置和工具的熟悉

  配置了Anaconda和pytorch环境，同时学习了一下cursor工具的使用（相比于之前的传统开发，确实很强大）同时通过查看python文档方式过了一部分语法，

- [x] Linux 相关技术和命令行的学习

  通过文档快速学习了一下相关常用命令的使用，后续可能计划学习docker和k8s相关容器技术部署项目 

- [x] 机器学习和深度学习相关知识的学习（主要通过老师推荐的资料来学习）

  通过阅读西瓜书学习了 (一  二 章),  感受书籍偏向于理论，刚开始会有点晦涩难懂的感觉，后续看一下结合相关博客理解一下

- [x] 阅读 预习将要讲解的论文 

  配置和熟悉一下zotero论文管理工具的使用，同时预习了一下论文 《Attention Is All You Need》（缺少了相关前置知识，导致阅读论文可能效率不高）

  预习了一下论文 《A Survey of the Usages of Deep Learning for  Natural Language Processing》

### 第二周(5.19~5.25)

- [x] 机器学习和深度学习相关知识的学习 (考虑一下进行代码的实践)

  主要还是对 python常用代码库的一个学习，同时对师兄论文代码的学习。

  由于还需处理学校毕业的相关的事务，整体的学习效率不高。

- [x] 阅读论文 

  本星期论文数较多，主要完成的任务还是对讲解论文的学习，同时可能因为相关基础知识的缺失，理解的较为吃力，后续再下一周再加强本星期论文的学习

### 第三周(5.26~6.1)

- [x] 加强对前面讲解论文的学习，同时学习新的论文。

  再复习之前的论文和学习新的论文的时候，之前学习的知识点很容易忘记，不禁让我思考，我是否真的学会这个知识点，还是说学习知识的速度更不上遗忘的速度，由于我从小到大并没有做笔记的习惯，不知是否是这个原因，同时由于老师发布了组会要讲论文的任务。所以在下一个星期，我打算all in 论文，重新规划一下论文的学习习惯，不以速度为主，以质量为主。

- [x] 争取把西瓜书推荐的章节再这个星期阅读完毕。

  再进行了西瓜书的 神经网络的章节的学习，熟悉了基本的工作原理，了解输入层和隐藏层和输出层的架构原理和反向传播的修改权重

  由于西瓜书的章节有些过于偏向数学理论，直接学习可能过于抽象，感觉学习效率不高，后续通过我之前学习技术的方式进行学习，通过查询相关的互联网的[技术博客](https://blog.csdn.net/illikang/article/details/82019945)来进行学习，

  后续再进行西瓜书强化学习的章节学习的时候，再查询相关技术博客，强化学习涉及的概念和数学知识相比于神经网络要更多，还需要时间继续沉淀

- [x] 进行相关机器学习代码实践。

  实践方面的话，主要还是查询之前的论文代码的学习，同时完成一个基础且经典的神经网络手写数字识别代码

### 第四周(6.2~6.8)

- [x] 看熟悉论文，改变一下学习论文方式，并产出相关论文笔记。

  着重理解了  Transformer，bert,gan-bert的论文 并产出对应的笔记。

### 第五周(6.9~6.15)

- [x] AAAMLP代码实践的学习

  学习了 AAAMLP的 “交叉检验” 和 “评估指标” 以及“组织机器学习项目” 这几个章节的学习，同时完成学习了“处理分类变量”的部分章节，主要是理解相关的概率和知识点，以及对工作代码的理解。

- [x] 理论知识的学习

  回顾复习一下之前的论文。

  学习一些 关于 强化学习的知识点。

### 第六周(6.16~6.22)

- [x] 离校相关事务处理（在校的最后一个星期）。

### 第七周(6.23~6.29)

- [x] 阅读相关  Flash Attention 论文

  通过阅读论文和学习相关技术博客，理解 flash attention如何通过 对注意力矩阵进行对应的分块技术 以及对中间数据优化来进行对应的 IO次数的 减少，以此来减少相应模型训练的时间。

- [x] 相关代码框架的学习

  主要对 llama.cpp 的环境的配置以及 代码的编译，期间各种的环境和版本问题还是花费了不少时间，有考虑过运用docker 相关的技术来简便部署，但又考虑到可能本地虚拟机可能对cuda的支持并不友好，后续打算通过 wsl2 以此来 实现docker技术的运用和cuda的支持会相对较好。花了一定的时间 配置和编译了cpu版本推理和gpu版本推理的 llama程序。通过llama.cpp在本地window部署 deepseek-r1 蒸馏的7B的模型，当然后续还得通过vllm以及其他框架 体会理解和实践一下完整流程。 

  ![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1daf5baacc414c6f828e254370a047ac~tplv-k3u1fbpfcp-watermark.image?) 

### 第八周(6.30~7.6)

- [x] 理论和实践的学习

    主要学习了LLM推理方面的知识点，学习目前LLM推理理论中主要的部分，以及相关的prefill阶段和decode阶段主要的相关的工作逻辑，同时了解目前的一些 目前主流 LLM推理框架，对于相关步骤的一些 优化，

     同时配置了一下wsl2 环境 以此来满足后续需要在本地linux 运行相关的 cuda的 需求，同时下载vllmini代码在本地配置环境wsl2进行本地运行和相关代码的学习

### 第九周(7.7~7.13)

- [x] 做了一些协助性的工作

###  第十周(7.14~7.20)

- [x] 理论和实践的学习

  学习了vllmini的的项目代码，同时记录了笔记的更新，项目以及笔记地址 https://github.com/LT0X/vllmini-learning，不断更新中，笔记的更新要考虑挺多方面，比我想象的要耗时。

### 第十一周(7.21~7.27)

- [ ] 理论和实践的学习